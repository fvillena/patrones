{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7.2-deep_learning_text_generation",
      "provenance": [],
      "authorship_tag": "ABX9TyP2GBx5F3vnWRa7ZuMfwCdB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fvillena/patrones/blob/main/7.2-deep_learning_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XdIgYpMWr_B"
      },
      "source": [
        "# Deep learning 2\n",
        "\n",
        "Deep learning es un tipo de aprendizaje de máquinas en donde las predicciones se realizan a través de una serie de operaciones matriciales concatenadas. Uno de los elementos más importantes en el Deep Learning es la casi nula ingeniería de características. En el ejemplo que veremos ahora, se utiliza un conjunto de imágenes segmentadas para realizar el entrenamiento de un modelo de segmentación automática de núcleos celulares, este entrenamiento simplemente se realiza con los pixeles crudos de las imágenes, sin ingeniería de características."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyDFW9FC-q-4"
      },
      "source": [
        "import tensorflow as tf # Biblioteca de redes neuronales\n",
        "import os # Módulo para interactuar con el sistema operativo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0e17pcmXHLR"
      },
      "source": [
        "Importamos el conjunto de datos. El conjunto de datos es una colección de diagnósticos médicos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCKnzmir--E8",
        "outputId": "2e0d98de-1b21-4d77-e98f-672f56aa0a60"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('corpus.txt', 'https://raw.githubusercontent.com/fvillena/workshopEmbeddingsAndClassifiers/master/corpus.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/fvillena/workshopEmbeddingsAndClassifiers/master/corpus.txt\n",
            "7397376/7394290 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvbmwCB8_Gvm"
      },
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8').lower() # Leemos el archivo descargado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zsTntNC_LNr",
        "outputId": "a56a199b-7126-4fa2-a95b-4e02ed79cd56"
      },
      "source": [
        "len(text) # Tamaño del corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7373422"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mM797bic_QCc",
        "outputId": "f9641786-6ec8-4e21-86e7-8fa1a74dab51"
      },
      "source": [
        "print(text[:250]) # Estos son los primeros 250 caracteres de nuestro corpus."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "celulitis y absceso de boca\n",
            "periodontitis cronica\n",
            "otras afecciones especificadas de los dientes y de sus estructuras de sosten\n",
            "trastornos de disco lumbar y otros, con radiculopatia\n",
            "celulitis y absceso de boca\n",
            "pitiriasis alba\n",
            "fisura anal\n",
            "periodontitis\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h04PrazXsSt"
      },
      "source": [
        "El vocabulario de nuestro conjunto de datos son todos los caracteres distintos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjOBzo7-_S53"
      },
      "source": [
        "vocab = sorted(set(text)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGsPfeOq_Vr2",
        "outputId": "ced3c3c1-5c24-4406-f03c-7ca9e2113a81"
      },
      "source": [
        "len(vocab) # Tamaño del vocabulario"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s7jlSGtYaNg"
      },
      "source": [
        "Transformamos el corpus a un conjunto de identificadores de caracteres."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COZZw0DL_bVa"
      },
      "source": [
        "# Esta función transforma una cadena de caracteres en una lista de identificadores de caracteres\n",
        "ids_from_chars = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=list(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ4l-Snk_7EE"
      },
      "source": [
        "# Esta función realiza el procedimiento inverso a la función anterior\n",
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_DcT3ru_o33"
      },
      "source": [
        "# Esta función concatena los caracteres en un string.\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "605ebhCN_rPj",
        "outputId": "6ded67cb-df70-4bff-9f95-720c60577b87"
      },
      "source": [
        "# Transformamos nuestro corpus.\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(7373422,), dtype=int64, numpy=array([24, 26, 33, ...,  3, 12,  2])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYFZAGEZ_tK_"
      },
      "source": [
        "# Transformamos los identificadores en un objeto que Tensorflow puede leer.\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9_jP5bSZVnC"
      },
      "source": [
        "Así se ven los primeros caracteres de nuestro conjunto de datos preprocesado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeVkmxAd_wlI",
        "outputId": "c3545209-44d4-4c5f-c5b1-ee078014a0cc"
      },
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "c\n",
            "e\n",
            "l\n",
            "u\n",
            "l\n",
            "i\n",
            "t\n",
            "i\n",
            "s\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIV9zrMwZmNp"
      },
      "source": [
        "Construiremos nuestro conjunto de datos para el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr5pDitt_yJw"
      },
      "source": [
        "seq_length = 100 # Esta es la cantidad de caracteres que tendrá cada uno de nuestros ejemplos.\n",
        "# examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O3lkBeaACuj"
      },
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True) # Generamos las secuencias de entrenamiento"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0JXklm1aRx6"
      },
      "source": [
        "Así se ve las primeras secuencias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2JFEksuaMFJ",
        "outputId": "b13df7de-eb23-4b9e-82aa-e9b9178a76be"
      },
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy().decode())\n",
        "  print(\"---\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "celulitis y absceso de boca\n",
            "periodontitis cronica\n",
            "otras afecciones especificadas de los dientes y de \n",
            "---\n",
            "sus estructuras de sosten\n",
            "trastornos de disco lumbar y otros, con radiculopatia\n",
            "celulitis y absceso d\n",
            "---\n",
            "e boca\n",
            "pitiriasis alba\n",
            "fisura anal\n",
            "periodontitis apical aguda originada en la pulpa\n",
            "osteomielitis, no\n",
            "---\n",
            " especificada\n",
            "examen de pesquisa especial para tumor del cuello uterino\n",
            "insuficiencia renal crónica\n",
            "i\n",
            "---\n",
            "nsuficiencia renal crónica\n",
            "periodontitis cronica\n",
            "herida de la muñeca y de la mano\n",
            "otros trastornos de\n",
            "---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk1Ogi9pAHVP"
      },
      "source": [
        "# Con esta función transformamos cada secuencia en textos de entrada y de salida para el entrenamiento.\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je7lwLHgAN_0"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0t9gbHHbUFh"
      },
      "source": [
        "Así se ve el texto de entrada y salida del primer ejemplo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdAd0nygAQFQ",
        "outputId": "adc3b3dd-c2e7-4c2a-a484-cd3f6f65f019"
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Entrada :\\n\", text_from_ids(input_example).numpy().decode())\n",
        "    print(\"---\")\n",
        "    print(\"Salida :\\n\", text_from_ids(target_example).numpy().decode())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entrada :\n",
            " celulitis y absceso de boca\n",
            "periodontitis cronica\n",
            "otras afecciones especificadas de los dientes y de\n",
            "---\n",
            "Salida :\n",
            " elulitis y absceso de boca\n",
            "periodontitis cronica\n",
            "otras afecciones especificadas de los dientes y de \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtByxqyMb2IM"
      },
      "source": [
        "Así preparamos el conjunto de datos para comenzar el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsNZDpvyAWT_",
        "outputId": "e853a50a-4594-4b34-db42-f43816c0b074"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znFZEaYbAYOF"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9gJbMaocIKr"
      },
      "source": [
        "Creamos la clase que definirá nuestro modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiIpsMQqAadU"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # Esta es la capa de embedding\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units, # Capa GRU\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size) # Esta es la capa de salida que predecirá el siguiente caracter.\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training) # Acá transformamos nuestra entrada en su representación de embedding.\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training) # Acá pasamos por GRU nuestra secuencia representada \n",
        "    x = self.dense(x, training=training) # Pasamos la salida de GRU por nuestra capa de clasificación\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8RkIx61czJW"
      },
      "source": [
        "Instanciamos nuestro modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5FCtQrmAc4z"
      },
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg8_l7ncdsNn"
      },
      "source": [
        "Estas son las dimensiones de un batch de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J43xDYxydHbY",
        "outputId": "9f1402c7-2fe4-46f0-c738-d895aa378aec"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 61) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghzTc7wjc74J"
      },
      "source": [
        "Así se ve la estructura de nuestra red"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0l2PK2upAgyz",
        "outputId": "771e74a9-6787-4d33-9884-ce31a53599af"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  15616     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  62525     \n",
            "=================================================================\n",
            "Total params: 4,016,445\n",
            "Trainable params: 4,016,445\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz3P2xQmd8to"
      },
      "source": [
        "Así se ve una predicción del modelo sin entrenar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mrn5kqoCAkNj",
        "outputId": "ade4d7b8-468a-4bd1-93b1-b5b24b2a57f9"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "print(\"Entrada:\\n\", text_from_ids(input_example_batch[0]).numpy().decode())\n",
        "print()\n",
        "print(\"Predicción:\\n\", text_from_ids(sampled_indices).numpy().decode())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entrada:\n",
            " sicion del diente\n",
            "colelitiasis\n",
            "pulpitis\n",
            "periodontitis apical aguda originada en la pulpa\n",
            "epilepsia\n",
            "d\n",
            "\n",
            "Predicción:\n",
            " 7z4w5eíèpnú9iyzzöújl.éjf67/öi3cfcu[UNK]l84í5ìsëka[UNK]í[UNK]ñ71[UNK]6bö[UNK]wúlñ.+9cìg jcvlìæ7s)i yfonjf ì)1èx4j4cw1pg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGyCfgubeNfi"
      },
      "source": [
        "Configuramos el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxTGMeNnAwIk"
      },
      "source": [
        "model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YqjzDF8AzgT"
      },
      "source": [
        "EPOCHS = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry_qZ0rZefmO"
      },
      "source": [
        "Ajustamos el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vsh3s0IIA4n0",
        "outputId": "b5753f24-bd94-4d96-be90-05e7ca4553fc"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1140/1140 [==============================] - 62s 53ms/step - loss: 1.3583\n",
            "Epoch 2/5\n",
            "1140/1140 [==============================] - 64s 55ms/step - loss: 0.3820\n",
            "Epoch 3/5\n",
            "1140/1140 [==============================] - 63s 54ms/step - loss: 0.3363\n",
            "Epoch 4/5\n",
            "1140/1140 [==============================] - 63s 54ms/step - loss: 0.3182\n",
            "Epoch 5/5\n",
            "1140/1140 [==============================] - 63s 54ms/step - loss: 0.3097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mmyw206A6Mn"
      },
      "source": [
        "# Con esta clase podemos generar texto de tamaño cualquiera data una entrada.\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['', '[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4BIBDA3A-G7"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afXKS0ocgNu-"
      },
      "source": [
        "Generamos texto sintético. Se puede observar que el modelo aprendió a generar una lista de diagnósticos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf0zjErmBAsm",
        "outputId": "2a758b72-4e0a-4b9a-b292-031aab045712"
      },
      "source": [
        "states = None\n",
        "next_char = tf.constant(['neoplasia']) # Esta es la secuencia de entrada.\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "print(result[0].numpy().decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "neoplasia, no especificada\n",
            "observacion por sospecha de enfermedad o afeccion no especificada\n",
            "observacion por \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtdTW_9NhqWi"
      },
      "source": [
        "Probamos con varias palabras de entrada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJASrENqBG2V",
        "outputId": "1807e5e6-a2e1-4af8-d08c-4ec4b0f20509"
      },
      "source": [
        "states = None\n",
        "next_char = tf.constant(['neoplasia', 'periodontitis', 'virus', 'infección', 'fractura'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "for r in result:\n",
        "  print(r.numpy().decode())\n",
        "  print(\"---\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "neoplasiadas\n",
            "observacion por sospecha de enfermedad o afeccion no especificada\n",
            "epistaxis\n",
            "observacion y evalua\n",
            "---\n",
            "periodontitis cronica\n",
            "prueba y ajuste de protesis dental\n",
            "embarazo confirmado\n",
            "embarazo confirmado\n",
            "episodio depresi\n",
            "---\n",
            "viruscirura del coca\n",
            "consulta no especificada\n",
            "consulta no especificada\n",
            "consulta no especificada\n",
            "consulta \n",
            "---\n",
            "infección12 no clasificada en otra parte\n",
            "embarazo confirmado\n",
            "periodontitis cronica\n",
            "consulta no especificada\n",
            "c\n",
            "---\n",
            "fracturaal esperada\n",
            "soplo cardiaco, no especificado\n",
            "consulta no especificada\n",
            "consulta no especificada\n",
            "consul\n",
            "---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOXpxZlGBUIz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}